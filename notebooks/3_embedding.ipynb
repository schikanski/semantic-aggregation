{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/schikanski/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from fiz_lernmodule.preprocessing import PreProcessor\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "seed_list = ['hair_dryer', 'video_codec', 'diesel', \"contact_lens\", \"contact_lens_us_c\", \"3d_printer\"]\n",
    "src_dir = '.'\n",
    "seeds = [seed_list[1]]\n",
    "seed_name = seeds[0]\n",
    "\n",
    "with open(src_dir + \"/data/\" + seed_name + \"/terms_attributes.pkl\", 'rb') as infile:\n",
    "    df = pickle.load(infile)\n",
    "architectures = [\"bert-base-uncased\",\n",
    "                'allenai/scibert_scivocab_uncased',\n",
    "                'google/pegasus-big_patent', # faulty\n",
    "                'google/bigbird-pegasus-large-bigpatent',\n",
    "                'AI-Growth/PatentSBERTa',\n",
    "                'distilbert-base-uncased']\n",
    "\n",
    "idx1 = 1\n",
    "checkpoint = architectures[idx1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    }
   ],
   "source": [
    "family_ids_to_filter = np.unique(df['family_id'])[np.unique(df['family_id'], return_counts=True)[1] > 1]\n",
    "rows_to_drop = np.empty((0, 1), int)\n",
    "for family_id in family_ids_to_filter:\n",
    "    rows_to_drop = np.append(rows_to_drop, df[df['family_id'] == family_id].sort_values('pub_num').index[1:])\n",
    "df = df.drop(rows_to_drop)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=False, return_tensors='pt')\n",
    "    inputs_ids = inputs['input_ids'][0]\n",
    "\n",
    "    size = inputs['input_ids'].shape[1]\n",
    "    if size > 512:\n",
    "        split_input_masks = []\n",
    "        split_attention_masks = []\n",
    "        for i in range(int(size / 512)):\n",
    "            split_input_masks.append(inputs['input_ids'][0, (i * 512):((i + 1) * 512)].reshape(1, -1))\n",
    "            split_attention_masks.append(inputs['attention_mask'][0, (i * 512):((i + 1) * 512)].reshape(1, -1))\n",
    "        if (size % 512) != 0:\n",
    "            split_input_masks.append(inputs['input_ids'][0, -(size % 512):].reshape(1, -1))\n",
    "            split_attention_masks.append(inputs['attention_mask'][0, -(size % 512):].reshape(1, -1))\n",
    "    if size > 512:\n",
    "        split_outputs = []\n",
    "        for a, b in zip(split_input_masks, split_attention_masks):\n",
    "            temp_dict = {'input_ids':a, 'attention_mask':b}\n",
    "            split_outputs.append(model(**temp_dict))\n",
    "            del temp_dict\n",
    "    else:\n",
    "        outputs = model(**inputs)\n",
    "        lhs = outputs['last_hidden_state']\n",
    "    if size > 512:\n",
    "        key = 'last_hidden_state'\n",
    "        lhs = torch.Tensor()\n",
    "        for obj in split_outputs:\n",
    "            lhs = torch.cat((lhs, obj[key]), dim=1)\n",
    "    lhs = lhs[:, 1:-1, :].detach().numpy()\n",
    "    lhs.shape\n",
    "    decoded_list = [tokenizer.decode(x) for x in inputs_ids][1:-1]\n",
    "    len(decoded_list)\n",
    "    hashtag_list = [idx for idx, val in enumerate(decoded_list) if '##' in val]\n",
    "\n",
    "    def fuse_hashtags():\n",
    "        to_fuse = hashtag_list[::-1]\n",
    "        list_of_indices = []\n",
    "        for idx in to_fuse:\n",
    "            temp_list = [idx]\n",
    "            former_element = idx - 1\n",
    "            while former_element in to_fuse:\n",
    "                temp_list.append(former_element)\n",
    "                to_fuse.remove(former_element)\n",
    "                former_element -= 1\n",
    "            temp_list.append(former_element)\n",
    "            temp_list = tuple(temp_list[::-1])\n",
    "            list_of_indices.append(temp_list)\n",
    "        return list_of_indices\n",
    "\n",
    "\n",
    "    hashtags_to_fuse = fuse_hashtags()\n",
    "\n",
    "    def fuse(pos_list, lhs, decoded_list):\n",
    "        for chain in pos_list:\n",
    "            first_pos = chain[0]\n",
    "            last_pos = chain[-1]\n",
    "            word = ''.join(map(lambda x: x.replace('#', ''), decoded_list[first_pos:last_pos + 1]))\n",
    "            vector = np.mean(lhs[0, first_pos:last_pos + 1, :], axis=0)\n",
    "            decoded_list[first_pos] = word\n",
    "            lhs[:, first_pos, :] = vector\n",
    "\n",
    "            for i in range(last_pos, first_pos, -1):\n",
    "                decoded_list.pop(i)\n",
    "                lhs = np.delete(lhs, i, axis=1)\n",
    "            \n",
    "        return lhs, decoded_list\n",
    "\n",
    "    lhs, decoded_list = fuse(hashtags_to_fuse, lhs, decoded_list)\n",
    "\n",
    "    return (lhs, decoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_sentence(text):\n",
    "    text = re.split('\\.|\\;', text)\n",
    "    sentences = [sentence for sentence in text if len(sentence) > 30]\n",
    "\n",
    "    inputs = []\n",
    "    input_ids = []\n",
    "    for sentence in sentences:\n",
    "        tokenized = tokenizer(sentence, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        inputs.append(tokenized)\n",
    "        input_ids.append(tokenized['input_ids'])\n",
    "\n",
    "    outputs = []\n",
    "    lhs = []\n",
    "    for single_input in inputs:\n",
    "        output = model(**single_input)\n",
    "        outputs.append(output)\n",
    "        lhs.append(output['last_hidden_state'][:, 1:-1, :].detach().numpy())\n",
    "\n",
    "    decoder = lambda inputs_ids: [tokenizer.decode(x) for x in inputs_ids[0]][1:-1]\n",
    "    decoder(input_ids[0])\n",
    "    decoded_sentences = list(map(decoder, input_ids))\n",
    "\n",
    "    hashtag_extractor = lambda decoded_sentence: [idx for idx, val in enumerate(decoded_sentence) if '##' in val]\n",
    "    hashtag_list = list(map(hashtag_extractor, decoded_sentences))\n",
    "\n",
    "    def fuse_hashtags(hashtag_list):\n",
    "        to_fuse = hashtag_list[::-1]\n",
    "        list_of_indices = []\n",
    "        for idx in to_fuse:\n",
    "            temp_list = [idx]\n",
    "            former_element = idx - 1\n",
    "            while former_element in to_fuse:\n",
    "                temp_list.append(former_element)\n",
    "                to_fuse.remove(former_element)\n",
    "                former_element -= 1\n",
    "            temp_list.append(former_element)\n",
    "            temp_list = tuple(temp_list[::-1])\n",
    "            list_of_indices.append(temp_list)\n",
    "        return list_of_indices\n",
    "\n",
    "    hashtags_to_fuse = list(map(fuse_hashtags, hashtag_list))\n",
    "\n",
    "    def fuse(pos_list, lhs, decoded_list):\n",
    "        for chain in pos_list:\n",
    "            first_pos = chain[0]\n",
    "            last_pos = chain[-1]\n",
    "            word = ''.join(map(lambda x: x.replace('#', ''), decoded_list[first_pos:last_pos + 1]))\n",
    "            vector = np.mean(lhs[0, first_pos:last_pos + 1, :], axis=0)\n",
    "            decoded_list[first_pos] = word\n",
    "            lhs[:, first_pos, :] = vector\n",
    "\n",
    "            for i in range(last_pos, first_pos, -1):\n",
    "                decoded_list.pop(i)\n",
    "                lhs = np.delete(lhs, i, axis=1)\n",
    "            \n",
    "        return lhs, decoded_list\n",
    "\n",
    "    lhs, decoded_sentences = zip(*list(map(fuse, hashtags_to_fuse, lhs, decoded_sentences)))\n",
    "\n",
    "    return (lhs, decoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanie(sentence_vectors):\n",
    "    sentence_embeddings = np.empty((0, 768))\n",
    "    for vector in sentence_vectors:\n",
    "        sentence_embeddings = np.append(sentence_embeddings, np.mean(vector, axis=1), axis=0)\n",
    "    return sentence_embeddings\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    return [item for sublist in a_list for item in sublist]\n",
    "\n",
    "def flatten_array(list_of_arrays):\n",
    "    new_array = np.empty((1, 0, 768))\n",
    "    for array in list_of_arrays:\n",
    "        new_array = np.append(new_array, array, axis=1)\n",
    "    return new_array\n",
    "\n",
    "\n",
    "def filter_text(text):\n",
    "    pre = PreProcessor()\n",
    "    filtered_text = pre.preprocess_text(' '.join(text), remove_short_long=True)\n",
    "    index = []\n",
    "    next_pos = 0\n",
    "    for word in filtered_text:\n",
    "        for idx in range(next_pos, len(text)):\n",
    "            if word == text[idx]:\n",
    "                index.append(idx)\n",
    "                next_pos = idx + 1\n",
    "                break\n",
    "    return index\n",
    "    \n",
    "with Pool(2) as pool:\n",
    "    output = pool.map(create_embedding_sentence, df['abstract_text'])\n",
    "    embeddings, decoded_text = zip(*output)\n",
    "    sentence_embeddings = pool.map(meanie, embeddings)\n",
    "    decoded_words = pool.map(flatten_list, decoded_text)\n",
    "    word_embeddings = pool.map(flatten_array, embeddings)\n",
    "    indices = pool.map(filter_text, decoded_words)\n",
    "\n",
    "temp1, temp2 = [], []\n",
    "for i in range(len(indices)):\n",
    "    temp1.append(np.take(word_embeddings[i], indices[i], axis=1))\n",
    "    temp2.append([decoded_words[i][x] for x in indices[i]])\n",
    "\n",
    "word_embeddings = temp1\n",
    "decoded_words = temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_embeddings'] = word_embeddings\n",
    "df['decoded_text'] = decoded_words\n",
    "df['decoded_sentences'] = decoded_text\n",
    "df['sentence_embeddings'] = sentence_embeddings\n",
    "df['document_embeddings'] = df['sentence_embeddings'].apply(lambda x: np.mean(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(src_dir + \"/data/\" + seed_name + \"/pre_embedding.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(df, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f1f4634a8703249e3b09704ef9f7cfb5dc67529cced6759ed0f0a6e08925072"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('pev2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
